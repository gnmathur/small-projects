Hi, I'm Toni Cowan-Brown. I'm Benedict Evans. It's nearing the end of the year, and you've done your annual presentation. Oh, I have, I have. Anyone who doesn't know, every year for kind of the last decade almost, I've done a big annual slide deck trying to sort of point out the key kind of structural trends, the key charts, the key questions, the key kind of break points and leverage points and issues and puzzles in what's happening in technology. And I think the first time I did it, it was a mobile first couple of times I did it. It was a mobile presentation, and then it kind of evolved into being a kind of technology platform, gatekeeping, e-commerce, advertising kind of a question, because that was what the question seemed to be. And now, of course, we have something new to talk about. And so I did a presentation that was mostly AI last year, and this year, I've done another one, which I've called AI Use the World. It's interesting you're presenting it that way, because I had it open of just like, okay, so your first presentation is 2013. And to exactly what you were saying, I noticed the same pattern of for the first three years, it was mobile as eating the world. Then it was all these other things around tech. Last year was AI, this year was AI. I was like, oh, is this the AI section, your new mobile section of just like it's so layered, it's actually so complicated, it's still so new, and we've got so many questions. But this annual presentation for me also is just like, how many slides can Benedict go for in 30 minutes? Yeah. Well, there's a sort of side conversation on how people do presentations and the style of talking and the style of presenting information and what you're trying to do and how you're trying to do it. Which I think we've probably done a podcast about that at some point. We've certainly done a podcast about charts. But that's actually a good point. What do you want people to leave? What do you want people to leave with once they've heard this 30-minute presentation? Well, you know, there's a joke about the objective of a consultant is to scare you, but not too much. Scare you enough to pay you more money to help you. Yeah, exactly. I'll tell you the Oscar and if you give me money, I'll tell you what to do about it. But here, what I'm trying to do, and as much as what I say about when I write, to write is to think. When I write, I'm trying to understand something. And what I'm trying to do here is sort of pull together, how can we get some sort of coherent, systematic understanding of what we're talking about when we say quote, unquote, AI, and even the phrase AI is kind of a problem in itself, which is sort of another sort of sidebar. What do you mean when you say AI? AI is an old joke, AI is whatever doesn't work yet. And so, but they're kind of, but when I was doing a presentation about e-commerce in like 2019 or 2020 or 2021, then it was a huge number of things happening, and let's try and point to all the different things that are going on, and all these kind of maturing spaces, and all these changes, and all these companies, and pick up some scenes of what's happening, and how you might place your company within that. Whereas the challenge in looking at generative AI is, it still seems to me that every conversation that one has about this, quite quickly turns into a set of puzzles of different people exchanging metaphors and mental frameworks to try and understand what this even is. Just a little bit like the internet in the mid-90s. What is this? What does this mean? How will this work? Which is quite different talking about e-commerce five years ago. You got it. You know what this was. It's like a level of playing field that just like, let's get everyone on the same page so that we understand what we're actually talking about and what the terms we're using actually means and where we are right now. There's that, but you can separate this out because there are, I get these daily weekly newsletters about AI, and here are the 15 year models that have been launched, and here's the 25 new things that are happening, and NVIDIA is doing this, and Tropic is doing that. So there's a huge amount of stuff happening. And I keep thinking that we're in the feeds and speeds phase of these models, which if people are old enough to remember, the PC is in the 80s and 90s, there would be all these great big, fat, thick computer magazines in the shops, and they would have like group test of, which set, here are 20 sound cards, which one should you buy depending on what you're doing, and here are 25 people selling you a 486DX 66 megahertz. Which one should you get? And the answer was that it kind of depends. And now you can do these kind of super complicated charts of benchmarks, and there's a new one, there's a new model every month or every week, and well, it scores higher on this and lower on that. And the new Amazon model, it sort of seems to be better on images, but it's not quite so good on text, and there's a bigger context for it, David. It's cheaper, it's like benchmarking, where you get these group tests of, here are 20 word processors, and here are 30 color inkjet printers. Which ones should you buy? And so there's a lot of like tangible practical stuff happening. And there's the same thing if you go and read like the Dylan Patel semi-analyst thing. Here is all the stuff that's happening in data center memory and interconnect and fiber, and water cooling, and what's going on? What does TSNC say about demand for the latest $25 million photolithography machines? There's all this stuff happening. But then the other side of it is, there's a couple of charts that I kind of put earlier in the presentation, which is, everybody's heard of this. Every big company has got a trial. No one's really using it very much, or not many people are using it very much. There's a small number of fields where you had this immediate obvious use case, like it's great for coding, it's great for marketing, it's great for brainstorming. Certain kinds of use cases, it's great for drafting emails or something. And then there's a lot of other people who look at it and think, well, I don't really know what to do with this. Yeah. And it's not clear what the product is, it's not clear what the platform is. I was having this conversation with somebody who was around building companies in the 90s, and he was saying like, is Chachi PT Netscape or is it the web browser or is it HTML or like? And I guess it depends who you ask and how they're using it. Is this the app store or the iPhone or 3G? What line of layers and building blocks are we looking at here? And what is it that you do with this? How do we think conceptually about what this will mean? And you can turn around and say, yeah, well done, that's all very good. But it's like the joke about the Frenchman who says that's all very well in practice, but does it work in theory? And you have to meanwhile go out and build stuff, which is fine, but what are you building? I was a student using the Internet in the mid 90s, and I remember there were 10 people trying to sell me a web browser, a web server that I would run on my PC. So you would share documents on a web server that you would run on your PC when it was connected to the Internet. So if you weren't turning your PC off or needed to use a telephone, people couldn't read your documents. Again, you're building something. What are you building? Are you building a thing that's going to get absorbed into Windows, like the web browser or TCPIP? There are people trying to sell TCPIP stacks. What are you building? What's this going to be? There were people doing e-commerce where you thought it's going to be a, there'll be like an online mall. So the shop, your local shopping mall would have like its own website, and then there will be a gap page, and you could go to like, you go to your local mall's website, and then there'd be a page for gap and a page for CVS, and you go to the gap page and buy, and today, it just sounds kind of ludicrous. It was just not clear at all how this was supposed to work. And we're kind of, this is sort of a long way of saying, like kind of what I've tried to do with the presentation is say, okay, this is a giant big deal. So sort of the top and tail, the top is, this is a huge deal, no one's quite sure what it is. And the tail is, meanwhile, all the other stuff that we were excited about 25 months ago before ChatGPT 3.5 launched are still there. Like Amazon has a $55 billion ad business, Shein is the world's largest apparel retailer, almost all of Instacart's profits come from advertising. Netscape spends more commissioning TV shows in the entire European TV industry. There's all the other stuff that's still there. But meanwhile, what is this AI thing? And here's a question for you. All of that stuff is there. Do you think that all of that stuff is also now being impacted by AGI, by NLMs, by AIs in general? Or do you think they sort of live in a separate vortex of, no, that's still happening regardless of what's happening over here, or are they colliding and they're actually impacting one another? Well, so you can get kind of microcosms of this. So there's a great study that Bain published a couple of weeks ago. Which was doing a survey in consumers, and asking what they think of generative AI tools that retailers have launched. And so it's like, Walmart has a generative AI tool, it has a chatbot. Amazon has a chatbot called, I don't know, Arthur or Edward or something, I forget, and there's like a button. Some posh name. And the general reaction of consumers is like you spent, they don't quite put it like this, but you spent 25 years building e-commerce interfaces, and now you give me this chatbot that doesn't remember what I bought, and doesn't know where I live. And everyone's kind of sprayed chatbots everywhere. Everyone has sprayed the project. This is kind of my point. Every consumer has tried this, but a very large portion of consumers, something like a third to a half of all consumers in the developer world have now tried this, but only like two, three, four percent of people are using it every day. Every big company has had a pilot, but only five, ten, fifteen percent have put anything into production. And that's not the whole thing. That's just like one. Have you got one thing in production? Yes, no, and that's like 20, 30 percent, 40 percent of companies have got one thing in production, never mind everything else. So everyone thinks this is, and this is kind of the same, every retailer has got a bunch of generative AI projects, but what? And so, and I think it's kind of come back to this. I mean, the structure of the presentation is I kind of try to group things into sort of three sets. There's a set that says, like, there's a sort of primary generative AI question is, how much bigger or better are these models going to get? Are they going to keep scaling? We don't know. No one knows. People have opinions, but no one knows. And we will build the five, four platform companies, four big cloud companies will spend something over $200 billion this year, which is up $80 or $90 billion from last year, on building data center, and NVIDIA's chart goes up and to the right. NVIDIA is supply constrained, Microsoft is supply constrained. Everyone is scrambling to get more compute, even to deploy the stuff that they have, and of course, to try and build more clusters to train bigger models. Will the models get better if you make them bigger? And can you do that? Can you get enough data? What are the engineering challenges to doing that, even if you have enough data? And so we've had in the last couple of weeks a flurry of people saying, yeah, it looks like the pre-training is slowing down, the low-hanging fruit is gone, the models aren't getting as better, as easily as they were in the past. But on a three-year view, we don't know at its core whether these models will keep getting better. And I ended that, and then you can talk about Meta trying to make this open-source and trying to make it a commodity, and Apple trying to get it onto the device, and also how much data there is and so on and so on. But I kind of ended that section with a slide that says, it's a quote from The Godfather, that one thing in life is certain, history taught us anything, it's that you can kill anyone. And the kind of cross out can kill anyone. And like the tech version is like, semiconductors are a cyclical industry, and commodity technology tends to get cheap, tends to go to marginal cost, and exciting, important new technologies tend to produce bubbles. And when I read stories about investment banking groups in New York, not having enough staff to manage all of the data center financing projects that they've got in the pipeline, I think, yeah, I've seen that movie before. We've seen that movie, we know that. We've been here before. We know how that works. And what matters for the rest of us is, like A, we don't know how much better these models will get. And so that prompts, well, it might be like this or it might be like that for a lot of questions. But in the meantime, there are now half a dozen companies that are sort of in the top bracket of best model. And you have the best model, you have the best cheap model, which is sort of 80 to 90 percent of the performance for sort of 5 percent of the cost. And then you have the best model you can fit onto the edge, onto a PC, onto a smartphone, at which point, of course, you're no longer paying inference costs. And all of those are moving targets all the time and changing all the time. This is a big story this week, as OpenAI have put their new model out at $200 a month. On the other hand, the more interesting one was Amazon has finally got their own Nova model out, which seems to be more or less as good as all the others. The interesting question is why it took them so long, but okay now, Apple's got its models, which is slightly off to one side. There's Google, there's Meta, there's Amazon, there's OpenAI, Anthropic, maybe Mistral, maybe Elon Musk's X, maybe a couple of other things, maybe even Microsoft will have its own model. Fine, what does this mean to the rest of us? This is commodity technology, what else happens? And so then the two second buckets I had were, okay, what are these models actually good for? And how do you deploy them? How do you build companies and products with them? Can I ask before we go into the two buckets, has it always been the case with this big tech evolutions that we can bucket? What we're seeing in terms of this seems to be like two buckets of inquiry, the future trajectory, the questions that we have, that we actually don't have the answers to, which is where is this going? How is it going to be used? And then all the questions about the current approach is like, how is everyone using it right now? How are we addressing this? How are we deploying this? How are we managing these technologies? Is it always true that we've got this like present bucket and then the future trajectory? Or is this unique to AI? So I think this- Because it feels like there's a lot of like, where's this going? How are we going to use this? But actually there's a lot happening right in front of us right now. Well, let's just have a look at how everyone's deploying it right now in this moment. Yeah, well, that was the joke about the Frenchmen, isn't it? The challenge is, what are you building? Are you building, are you real networks? Are you Pointcast? Are you Myspace? Are you all of these people who are early, but who are right, but too early? Are you all these people who were wrong and early? There were lots of people who proposed stuff in the 90s and the 90s that didn't work and wasn't the thing. Plenty of people who proposed stuff on the smartphone that didn't work and that wasn't the thing. There are people who proposed stuff that was the thing, but it was way too early. Just the wrong time, yeah. And yes, you can say, don't talk about this, just go off and build stuff. Well, that's fine, but build, build, build what? And you've got this kind of interesting split between people who say, well, these models, let's kind of nail it down to what these models are good for. These models are very good at automating certain kinds of things, but not others. They get stuff wrong. Or rather, what they do is they say, what would an answer to this question probably look like? That's about the best way I can come up with formulating it right now. And there is some class of the question where that's fantastically useful and you could never have automated that before and this unlocks enormous value. And there are other classes of the question where that means you can't use it. So you ask the thing, you ask it for legal precedent, or you ask it for tax advice, or you ask it for medical advice, and it might be right. It's probably right. I have a piece in draft where I got mid-journey to make me an image of Yodi doing data entry in a very boring drab office. The point being that the Yoda quote, do or do not, there is no try. There's a category of problem where there is no try. There is no best such thing as a better model. There is right or not right. But you can also look at that and say, well, this is classic disruption, that the new thing tends to be crap at the stuff that was important for the old thing, but then it does something different that's new. And if you test an LLM by asking it to do complex logic problems or mass-corruptible problems or do kind of highly specific data retrieval, it's kind of like testing an Apple II by asking if it has five nines uptime and can I use it to run a bank's backend? Well, no, it can't and it doesn't, but that's the wrong question. Can I replace my $5,000 software development workstation with an iPhone? No, but that's the wrong question. And there's a sort of similar thing here of like this is his new thing. It doesn't work very well yet even to do what it's going to do, which is like the Internet in 95. Like you couldn't put Photoshop in a browser in 95. You couldn't put Excel in a browser in 95. It took 10 years to do Excel and 20 years to do Photoshop. But meanwhile, it was useful for something else. But we're sort of trying to work out, like how do we think about error rates? How do we think about product? How do you think about, can you do it top down? Can you say, well, what industries have got lots of low labor productivity? So there's a bunch of people, economists, economists have tried to do these sort of macro data studies of which industries seem to have lots of people doing really boring grant work. It seems like something you could automate. Or you can kind of do it bottom up and you can talk about, well, what does it mean if you can create a video for free? Certain kinds of video for free. What does it mean if translation is free? What does it mean if software development becomes 50% cheaper? But the problem with both of those approaches is imagine trying to do that in 1995 for the Internet, you'd have got some of it. You might have got the newspapers are screwed, but you wouldn't have got Airbnb. You know, you might have got- That's why we can't just evaluate it on what's possible right now. Yeah. You might have said you would have correctly pointed out that it would take 10 to 15 years longer, 20 years before you could do TV on the Internet, but you wouldn't have realized you would have missed YouTube. So, those things are all kind of useful, but in the end, you know, if we lived in a world where you could look at a new and unformed technology and predict everything that it was going to do, then that wouldn't be it. We would live in a different universe. Like, that's not possible. But you can find kind of poke away and ask, well, how do we think about what this could be and what you might be able to automate with it? And we've had this conversation a lot of you and I sort of sitting and thinking, like, what do we use this for? Yeah. And it took me a while to realize, oh, wait, the reason why benchmarking in this space is so much more complicated than trying to evaduate traditional software is the diverse skill sets and use cases. A, there's that, like the diversity of how this can be used, but also what's actually possible today versus how this actually is going to be used in 10, 15 years just makes it incredibly complicated to try and figure out, A, how people are using it, what makes it good or bad. To your point, you might find one specific application and go, this is actually brilliant and genius for me and I'll look at it and go, I have no idea how I would ever use this. That's bucket number one. What are your two other buckets? Well, the first bucket is, well, it's Gail. The second bucket is, let's look at some of the challenges in trying to think about what it is that you can do with this and how you can think conceptually about what you might build. And then the third question is, okay, how do you build companies with this? How do you build products with this? And there's a sort of base case here, which is to say, how do big companies, how do we always deploy new technologies and new products? And you can talk about disruption theory, you can talk about classic big companies. Generally, what happens is, the incumbents try and make it a feature, and we try and integrate it into our existing workflows, and when we get a new tool, we try and make it fit what we already do. And then over time, you change what you do to fit the new thing. Over time, you move from simply using it to save money on the stuff you're already doing to unlock new things that you might be able to do, new businesses, new tasks, new ways of doing your job. And then the third step is that sometimes people use it to create, like to completely redefine the market. So step one is that the travel agent has a website. Step two is now you book with a hotel. Step three is that Airbnb changes what hotel means. To give a kind of very schematic example. And of course, that means three different things to different companies. Like the Internet was a much bigger deal for travel agents than it was for Boeing. For example, Boeing still make, or indeed reliance, like you still basically sell kept their cell phone tickets. And so that gets you to kind of classic, like is this like an Accenture kind of a question, or is this a Bain, BCG, McKinsey kind of a question? Is this a CIO question or a CFO question? And the answer is yes, it depends what kind of change you're looking at. And so Accenture is now doing a billion dollars a quarter in what they call generative AI bookings, doing pilot projects and building stuff for big companies. And some of that is just kind of classic IT procurement questions. Do you go with a big company or a small company? Do you have multi-vendors or one? Do you go with Google or do you go with Salesforce? How do you choose which use cases? Which budget does it come out of? Do you have to depreciate it? Do you have to write off your existing on-prem self? All these kind of basic IT procurement questions, which are why Cloud is still only 30% of enterprise workflows? And I use this stat quite often. It's crazy, actually. If you're in the tech industry, Cloud has been around for 30 years, 25 years, it goes back to Salesforce or longer, it's old, it's boring, it's done, but it's only a third of enterprise workflows. Because this stuff just takes a while, and if you're the CIA, why would you invest in moving people off fax workstations on to AWS? Like next year, next year, next year. And so some of this is it will just take time. I saw a friend of mine sent a shared an email with a few of us, I think it was a friend of mine, it might even be you, but I don't think it was you, who just got a news update that the way that they can contact their bank has now changed, the fax machine will no longer be in order, and you go, wow, didn't know that anyone was still sending fax machine, faxes in this day and age. But to your point, that change took, what, 20 years for someone to say, oh, it's time we send that email to tell everyone to no longer send a fax. Yes. We're pulling the plug. Yeah. So some of this is, it's a classic line, the future is here, but it's unevenly distributed and it takes a while. So some of this is also, how do you work out what those new things might be? And there's a sort of, this is the same thing for a software company. Some of this stuff will naturally be a feature. Some of it will be cell check. Some of it will be incorporated into existing software. Some of the Salesforce will just add that thing that can draw off to the sales email. Some of it will be people work out a way that they can do something, unbundle some task out of Salesforce and turn it into a new company, or unbundle some process out of Oracle or out of Excel, or out of email and make it a new company. And that's why the typical big company today has four or 500 SaaS apps. Even at only a third of their workflows, it's sort of four or 500 SaaS apps. If you look at data from Oracle or productive companies like that. And so this is sort of a base case that says, well, this will flow out like, it'll work like cloud, it will work like SQL, it'll work like databases. It will change how you do. Some of it will just automate stuff you're already doing. Some of it will be cost saving that will get competed away. Some of it will be new products with new revenue. Sometimes there will be new companies that have a new definition of the market and steal some of your customers or create a new market. And then in 15 years, there'll be another thing. Now, there's another view which comes back to the scaling question, which is to say, yes, but what if you can just ask the LLM to do the whole thing? And you don't have to believe in AGI to believe that these models, I mean, will these models get 10x, 100x, 1000x better? You don't have to believe that gets you to AGI to believe it does something. And so then there's a sort of a thesis that says like, the sort of base case is like proofread this email, and the mid case is you have new vertical tools that will do like, you know, review all of the 10x in the insurance industry and answer me questions about that. And then there's a higher case, which is to say, you know, I'm moving to Singapore, can you arrange it, please? Or buy me a house? That can do, or even potentially even more than that. And so this is the sort of the AI maximalist view that you might have way more tasks being pulled into software without needing to create tools to do it, without having to have way more pieces of software. So you'd have way more stuff that can be automated without needing the intermediate step of somebody saying, I'm going to make software that will automate that because the LLM will just be much more general purpose. And so that's the sort of a view that says, no, it won't just work like all of the stuff that we've done before. It will be way more than that. And the puzzle I always have here is a sort of obviously the science question, which is we don't know how much, how good these models will get. But there's a sort of much more basic question, I think, which is that with every normal wave of technology, it's not the user's job to work out what this is useful for. You know, you don't go to Home Depot and buy, Home Depot, Home Depot, and buy like a crate of 20 electric motors, and then come home and make a drill and a washing machine and a food processor. You know, you buy a sewing machine, you buy a washing machine, you buy a drill. And the same thing with, say, machine learning. Big companies don't buy machine learning. They don't even buy image recognition. Say you're for your credit card company, you don't buy sentiment analysis, you buy fraud detection software. And so the challenge with this is now you're sort of, the sort of thesis is like you like, just give everybody the prompt, and they'll like think of things to do with it. And you need to educate the users on what they could do and think of what they could ask. But like most people are just doing their job, and they don't sit and think, hey, this is particular repetitive thing that I'm doing every day that I could automate. This is actually, this has always been the challenge with the idea. There's this whole thesis of bottom-up enterprise software sales that you don't need to sell to the CIO. You make this amazing cool tool, and the users will discover it and use it. And what actually happens is that that always works for the first 5 percent of the market. But most users in most companies aren't sitting and thinking, how can I make my job? Oh, I could automate this task. Let me go and find a tool that will do it. And they're generally not allowed to either, particularly if you're in a regulated industry, but even if you're not. And they're not sitting and thinking about, how could I work out a better way of doing this job? They're just doing it. And the hard part of the enterprise software, the building the software, and the same thing for consumers. When you make the software, you first of all have to work out that this is a problem. And generally, you're kind of inventing a problem that no one has really realized was a problem before. No one have really noticed they were doing that. Sometimes they know they've got this problem pain point, but generally people don't know they've got that pain point. And so you work out that this is a pain point, and then you work out the right way of automating it. And you instantiate that in a product, and then you go to a bunch of CIOs or CMAs or heads of HR or whatever the group of people is, and then you spend a year persuading them that this is a problem and that they should be paying you to fix it. And the kind of view that this is general purpose software and it's general purpose tool that anyone can just use, I always feel like, go watch The Office and imagine saying, I'm just going to give co-pilot to Dwight and Fat Keith and David Brent. And so you don't need to use our accounting tools anymore. You could just use ChatGPT to do the invoice thing. Are you sure? Do you really think that's a good idea? It's interesting that you bring this up. I was in the middle of, I don't know if I should actually share this, but I was in the middle of renewing my O1 visa and obviously my lawyer is an immigration lawyer, not a business lawyer. Because I have a single member LLC, she was saying, it will be good for you to get a letter from your own LLC saying that you'll be employed by that LLC. So I was like, wait, you want me to write a letter on behalf of Toni Cowan-Brown saying that I'm going to hire Toni Cowan-Brown as a creator, that makes no sense. And she was like, well, I've seen a couple of clients do that. Anyway, I went to ChatGBT, asked a few things. But what was interesting is ChatGBT gave me a couple of really good answers of how I should structure this. But the last bit, I just found it, but the last bit of that ChatGBT answer was really interesting to me. It's like, would you like help drafting an authorization for you? And I was like, I hadn't even thought of that. I went to ChatGBT to help me understand the problem. Not in my wildest dreams did I think, oh wait, ChatGBT can also draft me the legal authorization that I will then use for my O1 visa application. And so I was just like, yeah, please go ahead, draft me an authorization, which it did. And then the next answer was like, who's the best person to sign this? And what are the loopholes? And what do I need to be careful of? And it was interesting. I went back to my lawyer and I was like, well, does this work? And she was like, oh, that's great. You have a business lawyer. I was like, yes. I was like, no, this is ChatGVT. This is this great kind of puzzle here because, I mean, I wrote something about this in the summer when I had a trip to India, and I had to get a visa. And this is quite a complicated thing of the different documents that you need. And so having done it and hence knowing exactly what I needed, then I went to ChatGVT and asked, and like it was mostly right, but it wasn't actually right. And this is sort of my, did I say this before or after we started recording, my Yoda point? I can't remember if we said this before or after we started recording. So I'm writing something about error rates, and I've got my journey to make me a picture of Yoda doing data entry in like a dingy, boring office. And the point of the quote is that most people, certainly anyone, any men will remember is Yoda says, do or do not, there is no try. And the problem here is that the model will try, and it will make a good answer. But there are some use cases where that's great, and there are some use cases where that's not- And this was a perfect example because I have a lawyer looking at it. She's like, I'm an immigration expert, not a business expert, but put something in front of me, and she can say, yes, this is exactly what I needed you to do. It's like a middle ground. So this is this question of how much is this, is this automation, is this the analogy I always gave is, it gives you infinite interns where it's great to have 10 interns, but you check their work. My husband says it's just for play, same, because I would normally go to my husband and go, what would you, how would you write this? And he's like, I don't know, ask church EPT. So he's great because he feels like I've got a second husband in the house that I can go and bore my questions with. So part of the zooming out, it's fascinating that we're still over two years in and we're still thinking, what is this for? And what exactly does it replace? And is it actually replacing anything? Or is it making something bigger and better? Or is it actually just replacing one small step? Or is it just not a confident, but just someone I can just go poke and ask for guidance on something can come back? And I said someone, it's not someone, it's something, it's weird. And is that shifting habits? Is that the product changing? Is it new UX? Is it instantiating it in different ways? And you see some of this with OpenAI and Anthropic hiring head product officers, and OpenAI just hiring a marketing officer. Kevin Weill ran product Twitter and then ran product on Instagram. Mike Krieger founded Instagram and ran product there, and he's gone to Anthropic. Perplexity has come up with this sort of, theoretically, perplexity isn't doing anything that you couldn't do with OpenAI. But it said, well, let's narrow it and make it more specific and build product around that and make a UX around that and communicate to the users. This is what it's for, not those other things. And go back to the early web. The original web browser from Tim Berners-Lee was also a web server and an editor, because he thought that this was like a network drive and this was document sharing. And so you should be editing the doc, you would navigate to a document and then you would edit it. And this was better than browsing through a folder structure and then opening Word. And yes, I said earlier like you'd leave your PC on and that was the server. And then it emerged, no, this is a publishing system, not a sharing system, not a document sharing system. And then it took like five picket, what would pick your kickoff date for really being able to build apps in the browser, you could maybe say Salesforce or you could say Gmail. Maybe Gmail would be like certainly the next generation. And the whole thing of Ajax, which you have to be a certain age to remember the idea of Ajax, when you would like you wouldn't have to hit reload, you wouldn't wait for the page to reload, you could just do stuff and stuff on the page would change. But to pick a number 10 years, 7, 10, 15 years, depending on what you're thinking about, to make stuff that was like your desktop app that worked in a web browser. That evolution talk. And so it took quite a long time for it to become where it was going to end up. And then of course, Apple says, no, we're going to make it apps again. It turns the whole thing upside down. And we're still at that moment of like, did the web browser have an editor? Did the web browser have a web server? Did it have payment? Mark and reason always says, what is it 405, 406, the error code that says payment. There was a place 404 is not found, and then I think it's 403, I should know this. One of those error codes that's not 405 or 404 or something else, it was a placeholder for payment. Because they thought they would just kind of, well, obviously, well, we'll just add payment to this, and that turned out not to work, and that takes you to Bitcoin and other things. But zooming out to the presentation, what I kind of tried to do is pull together and say, look, here are these charts of what's happening. Here are these questions around these charts of what people are using it for, what they're not using it for, what they think about it, what they don't think about it. How this stuff tends to work, how it's worked in the past, how it might work, what are the questions of where it might go? And I think one of the final slides in the sort of the AI section is a framing that says, like, pause it, all AI questions have one of two answers. The answer is either it will work exactly like every other platform shift. Like, what budget should just come out of? Should we hire Accenture? Should we hire Bain? Should we work with Microsoft or Google? Like, well, how did you do that? So the whole class of questions where the answer is, it's exactly like every other platform shift. And there's a whole other class of questions where the answer is, no one knows. Like, will open AI, will the LLMs go to the top of the stack? Will the chatbot be the UI? Is natural language the right interface? How good will the models get? How many models will there be? Will they be commodities? Will they be able to run them on your phone? Will they always have marginal cost? Will they keep scaling? Will the Chinese take over the whole thing? When does Microsoft ship their own model? They're like, no, we're not. To wrap it up, the title that you gave to your presentation is AI is the world, which is obviously a prey on the famous software is the world. Why was that the title that you went with? Oh, so software is the world was actually an essay written by my old boss, Mark Andreessen, which was a nice piece of content marketing. When did he write that? 2012? OK. It was when the firm was near, maybe earlier. So it's a decade old. Wow. Software is eating the world. 2011. Software is eating the world. It was an essay. It was a thing that they placed in the Wall Street Journal. And then it's on the website as well. And the thesis was basically that software was going to expand out of selling software as software to big companies and instead to be a thing that would change what user experiences were. And the canonical examples here are always Airbnb and Uber. Airbnb doesn't sell software to hotels. Uber doesn't sell software to taxis. It gets a little bit more difficult to think of a third example, ironically. I see you pausing for a second. It's so easy to cite those two. But the underlying thesis was software is just going to be way, way bigger. And the start opportunity is going to be way, way bigger than it was in the past because of cloud, because of mobile, because so many more people are online. We can build a much bigger venture capital firm, and so help us raise more bigger funds. And that worked very well. But it's a nice phrase. And I thought, well, in 2013 or something, I co-opted it to say, well, mobile is eating the world because at that point, it was not apparent that mobile was going to become the dominant platform, was replacing the PC as the dominant platform. And for years afterwards, people would still be arguing, even like you can still just about find people who will argue that, no, no, no, the PC is the platform and smartphones are just for consuming, as though these people have never heard of Instagram or TikTok or social media or anything else. But I think most of us understand that the smartphone became the dominant computer platform. And so that was what that phrase was intended to communicate, and then it was 100 slides talking about that. And well, when I say now, what's the next platform? So it feels like everyone sort of feels like this is the next platform, then the question is, well, yeah, but what does that mean? Is it the next platform the way cloud or mobile with the next platform? And like we've got 10, 15 years of building companies around it, and then there'll be something else. Is it, no, this is more like the new, and this is kind of the deck I did last year, is to say, is this a platform shift or is it more than a platform shift? We still have no idea. Because even though we have no idea on most things, we do kind of agree based on what you've just shared, is that this is a new type of platform and it is big in that respect. What it becomes, how it's used, how it evolves has yet to be decided. But you've bucketed it. It's interesting, just like we spoke about software, mobile, and now this kind of AI. Yeah, I love that you've spent too long in California. You just said, based on what you've just shared. Oh, stop it. We need to get you back to Belgium. Yeah, I mean, there's this sort of general sense of, this is very big pause. Why do you see it exactly? So like you talked to investors now, it's like, we can't write a billion dollar check to Anstropic. So, but we also don't want to write the invest in a thin GPT wrapper. But then no one says Snapchat is a thin AWS wrapper. Is it Snapchat, AWS, or Google Cloud? I can't remember. Anyway, when you use a SaaS app, you don't say, well, this is just a thin AWS wrapper. So, do we end up with a whole, I mean, one of the things that I've used in the deck is, and in a sense, an awful lot of this is questions, not answers. But one of the questions I posed or the ways of posing a question was, do LLMs just become another API call? You use software, you build a UI, you've got data storage, you've got image recognition, you've got sentiment analysis, you've got voice recognition, you've got network calls, you've got a payment API to stripe, and you've got an LLM that can synthesize data out of this. And it's just another API call that you use to build software. Or does the API, does the LLM sit on top and make everything else an API call? And you're using the LLM as this new layer of abstraction and everything else sits further down the stack. My inclination is to say, well, and obviously the reality is always less tidy than these kind of neat syllogisms. And so the reality will be, there'll be like 10 different LLMs that you use, each of which will be like, so it will never be as neatly split as that. But that's sort of some of the question is, does this stuff sort of get abstracted away and you won't necessarily, half the time you're using it, you won't know you're using it, just as half the time you're using machine learning now, you don't know that you're using it and don't care. Like, I mean, we're doing this, you know, on recording this over a FaceTime call. And, you know, there's like 15 different machine learning algorithms happening. There's noise cancellation and, you know, image adjustment and, you know, it's doing portrait mode on the video. And we don't say, oh, I'm doing an AI call now. In fact, this is like the final slide. The final slide in the deck is that I went and compiled data from the US Census on how many people were employed as elevator attendants in the USA. This is the thing I love talking about, because it used to be that an elevator is like basically a vertical tram. Public transport is a streetcar. There's a driver, there's an accelerator and a brake, except it's private, it's inside a private building. And then in the 50s, you get automatic elevators. And you can go and pull the advertising. There's something called an autotronic elevator from Otis. And it's got electronic politeness, which is what it means, the infrared thing that stops the door closing on you. And it's an automatic elevator. And they're kind of like what I say, like when's the last time you took an automatic elevator? It just disappears, you don't think about it anymore. When's the last time you did autocorrect? You don't, it just happens. When's the last time you used the cloud? It just happens. And so that's sort of the base case is that's how this will evolve. But check back in five years and we'll see. We have to, exactly, we have to wait for it first. Your slides and your slash presentation is on your website, isn't it? They're both up there. And so it's great. Now you have the opportunity of going through the 100 slides in silence, watching your presentation or listening to this podcast episode. It feels like a good place to wrap up. That sounds good. And then we'll think of something else to speak about that isn't AI. Yeah, right. I was going to say, we know it's going to be AI. It just depends what it's going to be. Okay. Speak to you later. Bye.